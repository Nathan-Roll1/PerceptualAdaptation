---
title: "Replication of 'Generalized perceptual adaptation to second-language speech: Variability, similarity, and intelligibility' by Bradlow, Bassard, & Paller (2023, J. Acoust. Soc. Am.)"
author: "[Nathan Roll] (nroll@stanford.edu)"
date: "`r format(Sys.time(), '%B %d, %Y')`"
format:
  html:
    toc: true
    toc_depth: 3
---

## Introduction

This study is a partial replication of Bradlow, Bassard, and Paller's (2023) investigation of perceptual adaptation to second-language (L2) speech. The original study examined the conditions that facilitate or constrain perceptual adaptation to L2 speech, with a focus on how variability in training exposure affects generalization of this adaptation. The authors tested whether high-variability (i.e., multiple-talker) training is necessary for cross-talker generalization or whether low-variability (i.e., single-talker) training can be sufficient for such generalization.

This replication focuses specifically on the training phase of the original study. I aim to test the prediction that some single-talker training conditions can lead to significant improvement in L2 speech recognition, suggesting that high-variability exposure is not necessary for perceptual adaptation. Furthermore, I will examine whether training phase intelligibility plays a crucial role in perceptual adaptation to L2 speech, as suggested by the original study's post-hoc analysis.

## Methods

### Power Analysis

The original study found various effect sizes across different training conditions. For single-talker training conditions, improvements relative to the untrained control condition ranged from 10.2% (FAR training with BRP test talker, p < 0.003) to non-significant negative effects (-4.6% for TUR training with FAR test talker).

```{r power-analysis, message=FALSE, warning=FALSE}
# Load required packages
library(pwr)
library(tidyverse)
library(knitr)

# Convert percentage improvements to Cohen's d
# Based on original study's significant effects
# We're using an estimated pooled SD of 0.15 based on the reported SEs in the original study
calculate_d <- function(percent_improvement, pooled_sd = 0.15) {
  # Convert percentage to proportion
  prop_improvement <- percent_improvement / 100
  # Calculate Cohen's d
  d <- prop_improvement / pooled_sd
  return(d)
}

# Effect sizes from original study (significant results)
effects <- tibble(
  condition = c("FAR-BRP", "SPA-BRP", "BRP-BRP", "FAR-FAR", "FAR-SPA", "FAR-TUR"),
  percent_improvement = c(10.2, 6.8, 5.9, 5.6, 5.5, 5.4),
  cohens_d = calculate_d(percent_improvement)
)

# Calculate required sample sizes for different power levels
power_analysis <- effects %>%
  mutate(
    n_80_power = map_dbl(cohens_d, ~pwr.t.test(d = ., power = 0.8, sig.level = 0.05, type = "two.sample")$n),
    n_90_power = map_dbl(cohens_d, ~pwr.t.test(d = ., power = 0.9, sig.level = 0.05, type = "two.sample")$n),
    n_95_power = map_dbl(cohens_d, ~pwr.t.test(d = ., power = 0.95, sig.level = 0.05, type = "two.sample")$n)
  ) %>%
  mutate(across(starts_with("n_"), ceiling))

# Display the power analysis results
kable(power_analysis, 
      col.names = c("Condition", "% Improvement", "Cohen's d", 
                   "N (80% power)", "N (90% power)", "N (95% power)"),
      digits = c(0, 1, 2, 0, 0, 0))

# Calculate average sample sizes
avg_n_80 <- ceiling(mean(power_analysis$n_80_power))
avg_n_90 <- ceiling(mean(power_analysis$n_90_power))
avg_n_95 <- ceiling(mean(power_analysis$n_95_power))

# Create power curve plotting data manually to avoid warnings
# For minimum, mean, and maximum effect sizes
d_values <- c(min(effects$cohens_d), mean(effects$cohens_d), max(effects$cohens_d))
n_values <- seq(10, 100, by = 2)

# Create an empty data frame to store results
power_curve_data <- data.frame()

# Calculate power for each combination of d and n
for(d in d_values) {
  for(n in n_values) {
    # Need to ensure n is at least 2 for pwr.t.test to work
    if(n >= 2) {
      result <- pwr.t.test(n = n, d = d, sig.level = 0.05, type = "two.sample")
      power_curve_data <- rbind(power_curve_data, 
                                data.frame(d = d, n = n, power = result$power))
    }
  }
}

# Create labels for the plot
d_labels <- sprintf("d = %.2f", d_values)
names(d_labels) <- as.character(d_values)

# Create the power curve plot
ggplot(power_curve_data, aes(x = n, y = power, color = factor(d))) +
  geom_line(linewidth = 1) +
  geom_hline(yintercept = c(0.8, 0.9, 0.95), linetype = "dashed", color = "gray") +
  scale_y_continuous(breaks = seq(0, 1, 0.1), limits = c(0, 1)) +
  scale_color_viridis_d(name = "Effect Size", labels = d_labels) +
  labs(x = "Sample Size (per group)", y = "Power", 
       title = "Power Analysis for Different Effect Sizes",
       subtitle = "Based on significant effects from Bradlow et al. (2023)") +
  theme_minimal() +
  geom_vline(xintercept = 30, linetype = "dotted", color = "red")

# Annotate power at n=30 for average effect size
avg_effect <- mean(effects$cohens_d)
power_at_30 <- pwr.t.test(n = 30, d = avg_effect, sig.level = 0.05, type = "two.sample")$power
cat(sprintf("Power at n=30 for average effect size (d=%.2f): %.1f%%\n", 
            avg_effect, power_at_30*100))
```

### Planned Sample

Based on my power analysis, I plan to recruit 376 L1 American English listeners (47 per condition × 8 conditions) for this study. This sample size provides 90% power to detect the largest effects observed in the original study (d=0.68), though it offers more limited power (approximately 50-60%) for detecting the average effect sizes. Participants will be between 18 and 35 years of age, with no self-reported deficits in speech, language, or hearing, and with normal or corrected-to-normal vision. I will recruit participants through Prolific, compensating them for their participation in a single session focusing on the training phase with 120 stimuli per participant (double the original study's 60 stimuli). While this sample size represents a compromise between statistical power and feasibility constraints, the increased number of stimuli per participant may enhance measurement precision relative to the original study, potentially offsetting some of the power limitations.

### Materials

While the original study used materials from the ALLSSTAR Corpus, our replication will use sentence recordings from the L2-ARCTIC corpus due to its accessibility and comprehensive documentation. The L2-ARCTIC corpus includes recordings from 24 non-native speakers of English from six L1 backgrounds (Hindi, Korean, Mandarin, Spanish, Arabic, and Vietnamese).

For our study, I will select four L2 talkers from four different L1 backgrounds (one Spanish, one Hindi, one Korean, and one Mandarin), with two males and two females. These talkers will be selected based on moderate-to-good comprehensibility and distinct L2-accented speech, similar to the criteria used in the original study.

I will compile a set of 120 simple sentences (as opposed to the original 60) from each of the four talkers. In both the training and test phases, the sentences will be digitally mixed with speech-shaped noise at a fixed signal-to-noise ratio (SNR) of 0 dB to match the original study's procedure.

### Procedure

Similar to the original study, participants will listen to sentence recordings over headphones or earbuds. The sentences will be presented one at a time with no possibility of repetition. Participants will type what they heard using the computer keyboard before advancing to the next sentence. No feedback will be provided.

Participants will be randomly assigned to one of eight training conditions or the control group:

1. Four single-talker conditions (ST-Spanish, ST-Hindi, ST-Korean, ST-Mandarin), where all 120 sentences are produced by a single talker.
2. Four multiple-talker conditions, where three of the four talkers produce 40 sentences each, with one talker excluded from the training set (noSpanish, noHindi, noKorean, noMandarin).
3. An untrained control group that will not participate in any training but will take the final intelligibility assessment.

All sentence transcriptions will be scored using Autoscore or a similar automated scoring tool, which counts a sentence as either correctly (score = 1) or incorrectly (score = 0) recognized if the transcription exactly matches the script.

Unlike the original study, I will not have a separate test phase with an 11-hour delay. Instead, I will analyze participants' performance throughout the training phase, dividing the 120 sentences into quartiles of 30 sentences each to track improvement over time.

### Analysis Plan

Can also quote directly, though it is less often spelled out effectively for an analysis strategy section.  The key is to report an analysis strategy that is as close to the original - data cleaning rules, data exclusion rules, covariates, etc. - as possible.  

**Clarify key analysis of interest here**  You can also pre-specify additional analyses you plan to do.

### Differences from Original Study

Explicitly describe known differences in sample, setting, procedure, and analysis plan from original study.  The goal, of course, is to minimize those differences, but differences will inevitably occur.  Also, note whether such differences are anticipated to make a difference based on claims in the original article or subsequent published research on the conditions for obtaining the effect.

### Methods Addendum (Post Data Collection)

You can comment this section out prior to final report with data collection.

#### Actual Sample
  Sample size, demographics, data exclusions based on rules spelled out in analysis plan

#### Differences from pre-data collection methods plan
  Any differences from what was described as the original plan, or “none”.


## Results


### Data preparation

Data preparation following the analysis plan.
	
```{r include=F}
### Data Preparation

#### Load Relevant Libraries and Functions

#### Import data

#### Data exclusion / filtering

#### Prepare data for analysis - create columns etc.
```

### Confirmatory analysis

The analyses as specified in the analysis plan.  

*Side-by-side graph with original graph is ideal here*

### Exploratory analyses

Any follow-up analyses desired (not required).  

## Discussion

### Summary of Replication Attempt

Open the discussion section with a paragraph summarizing the primary result from the confirmatory analysis and the assessment of whether it replicated, partially replicated, or failed to replicate the original result.  

### Commentary

Add open-ended commentary (if any) reflecting (a) insights from follow-up exploratory analysis, (b) assessment of the meaning of the replication (or not) - e.g., for a failure to replicate, are the differences between original and present study ones that definitely, plausibly, or are unlikely to have been moderators of the result, and (c) discussion of any objections or challenges raised by the current and original authors about the replication attempt.  None of these need to be long.
